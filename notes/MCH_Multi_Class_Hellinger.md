# Multi-Class Hellinger (MCH)

## اطلاعات کلی
- **منبع اصلی:** Hellinger Distance Trees for Imbalanced Streams - Lyon et al.
- **سال معرفی:** 2014 (نسخه باینری)، 2022 (نسخه چندکلاسه)
- **نویسندگان:** Lyon et al., Dong M., Liu M., Jing C.
- **انگیزه:** حل مشکل عدم تعادل کلاس‌ها و ایجاد تقسیم‌های مقاوم در برابر skew
- **مزیت کلیدی:** بی‌حساسی به توزیع کلاس‌ها و مقاومت در برابر داده‌های نامتعادل

## هدف و کاربرد

معیار Multi-Class Hellinger تعمیم چندکلاسه فاصله Hellinger باینری است که برای رفع مشکلات زیر طراحی شده:

1. **مقاومت در برابر عدم تعادل کلاس‌ها** - نادیده گرفتن prior کلاس‌ها
2. **اندازه‌گیری تفاوت توزیع‌ها** - محاسبه تفاوت توزیع احتمال کلاس‌ها
3. **عملکرد بهتر در داده‌های skewed** - مقایسه با Gini و Entropy

این معیار مخصوصاً برای **مسائل چندکلاسه نامتعادل** طراحی شده و بدون نیاز به تبدیل به مسائل باینری عمل می‌کند.

## فرمول ریاضی

### فرمول اصلی
$$\text{MCH} = \frac{1}{2}\sum_{j=1}^{k}\left(\sqrt{p_{Lj}} - \sqrt{p_{Rj}}\right)^2$$

### تعریف متغیرها

| نماد | تعریف |
|------|--------|
| **$\text{MCH}$** | امتیاز معیار Multi-Class Hellinger |
| **$k$** | تعداد کلاس‌های موجود |
| **$p_{Lj}$** | احتمال کلاس $j$ در شاخه چپ |
| **$p_{Rj}$** | احتمال کلاس $j$ در شاخه راست |
| **$n_{Lj}$** | تعداد نمونه‌های کلاس $j$ در شاخه چپ |
| **$n_{Rj}$** | تعداد نمونه‌های کلاس $j$ در شاخه راست |

### محاسبات تکمیلی

**احتمالات کلاس در هر شاخه:**
$$p_{Lj} = \frac{n_{Lj}}{n_L}, \quad p_{Rj} = \frac{n_{Rj}}{n_R}$$

جایی که $n_L$ و $n_R$ تعداد کل نمونه‌ها در شاخه‌های چپ و راست هستند.

## مثال محاسبه

فرض کنید در یک گره 100 نمونه از سه کلاس A, B, C داریم:

| شاخه | A | B | C | مجموع |
|------|---|---|---|-------|
| چپ  | 20| 5 | 15| 40    |
| راست| 10| 25| 25| 60    |

### گام‌های محاسبه:

**گام 1:** محاسبه احتمالات شاخه چپ
- $p_{L1} = 20/40 = 0.5$, $p_{L2} = 5/40 = 0.125$, $p_{L3} = 15/40 = 0.375$

**گام 2:** محاسبه احتمالات شاخه راست
- $p_{R1} = 10/60 = 0.167$, $p_{R2} = 25/60 = 0.417$, $p_{R3} = 25/60 = 0.417$

**گام 3:** محاسبه تفاضل جذر احتمالات
- کلاس A: $(\sqrt{0.5} - \sqrt{0.167})^2 = (0.707 - 0.408)^2 = 0.089$
- کلاس B: $(\sqrt{0.125} - \sqrt{0.417})^2 = (0.354 - 0.646)^2 = 0.085$
- کلاس C: $(\sqrt{0.375} - \sqrt{0.417})^2 = (0.612 - 0.646)^2 = 0.001$

**گام 4:** محاسبه نهایی
$$\text{MCH} = \frac{1}{2}(0.089 + 0.085 + 0.001) = 0.0875$$

## ویژگی‌های فنی

- **پیچیدگی محاسباتی:** $O(k)$ جایی که $k$ تعداد کلاس‌هاست
- **محدوده مقادیر:** $0 \leq \text{MCH} \leq 1$
- **حساسیت به نامتعادلی:** پایین - مقاوم در برابر skew
- **مناسب برای:** مسائل چندکلاسه نامتعادل

### تفسیر مقادیر:
- **$\text{MCH} = 0$**: توزیع کلاس‌ها در دو شاخه یکسان (تقسیم بی‌فایده)
- **$\text{MCH} = 1$**: هر کلاس فقط در یک شاخه ظاهر می‌شود (تقسیم ایده‌آل)

## مقایسه با سایر معیارها

| جنبه | MCH | Gini | Entropy | Twoing |
|------|-----|------|---------|--------|
| **حساسیت به skew** | بی‌حساس | حساس | حساس | متوسط |
| **مقاومت در برابر outlier** | بالا | متوسط | پایین | متوسط |
| **عملکرد در داده نامتعادل** | عالی | ضعیف | ضعیف | خوب |
| **پیچیدگی محاسبه** | پایین | پایین | متوسط | متوسط |
| **قابلیت تفسیر** | متوسط | بالا | متوسط | بالا |

## مزایا

### 1. مقاومت در برابر عدم تعادل
- نادیده گرفتن prior کلاس‌ها باعث عملکرد یکسان در همه کلاس‌ها می‌شود
- برای داده‌هایی با کلاس‌های اقلیت بسیار مفید است

### 2. پایداری عددی
- استفاده از جذر باعث کاهش تأثیر مقادیر بسیار کوچک یا بزرگ می‌شود
- مقاوم در برابر نویز و outlier

### 3. تعمیم طبیعی از حالت باینری
- برای $k=2$ به فاصله Hellinger کلاسیک تبدیل می‌شود
- عدم نیاز به تبدیل مسئله چندکلاسه به چند مسئله باینری

## محدودیت‌ها

### 1. کمتر شناخته‌شده
- نسبت به Gini و Entropy کمتر مطالعه شده است
- مراجع و مستندات محدودتری دارد

### 2. تفسیر پیچیده‌تر
- مفهوم فاصله Hellinger برای مبتدیان دشوار است
- نیاز به درک مفاهیم آماری پیشرفته‌تر

### 3. حساسیت به شاخه‌های خالی
- اگر شاخه‌ای هیچ نمونه‌ای نداشته باشد، محاسبه مشکل‌ساز است
- نیاز به مدیریت خاص موارد خاص

## نکات پیاده‌سازی

### مراقبت‌های ضروری:
- **شاخه‌های خالی:** اگر $n_L = 0$ یا $n_R = 0$، مقدار $\text{MCH} = 0$ برگردانید
- **دقت عددی:** از overflow در محاسبه جذر جلوگیری کنید
- **کلاس‌های غایب:** کلاس‌هایی که در هیچ شاخه نیستند را نادیده بگیرید

### بهینه‌سازی:
- محاسبه جذر احتمالات فقط یک بار
- کش کردن محاسبات تکراری برای کلاس‌های مختلف
- استفاده از تابع sqrt بهینه‌شده NumPy

## کد شبه

```python

def multi_class_hellinger(y_left, y_right):

"""

محاسبه امتیاز معیار Multi-Class Hellinger

Parameters:
y_left: آرایه برچسب‌های کلاس برای شاخه چپ
y_right: آرایه برچسب‌های کلاس برای شاخه راست

Returns:
MCH: امتیاز Multi-Class Hellinger (بین 0 و 1)
"""

# محاسبه تعداد نمونه‌ها
n_left = len(y_left)
n_right = len(y_right)

# بررسی شرایط خاص
if n_left == 0 or n_right == 0:
    return 0.0

# یافتن کلاس‌های موجود
classes = unique(concatenate([y_left, y_right]))

# محاسبه مجموع مجذور تفاضل جذرها
hellinger_sum = 0.0

for class_j in classes:
    # احتمال کلاس در هر شاخه
    p_Lj = sum(y_left == class_j) / n_left
    p_Rj = sum(y_right == class_j) / n_right
    
    # اضافه کردن مجذور تفاضل جذرها
    hellinger_sum += (sqrt(p_Lj) - sqrt(p_Rj)) ** 2

# محاسبه نهایی MCH
return 0.5 * hellinger_sum



```
## منابع
- Lyon, R.J., et al. (2014). Hellinger Distance Trees for Imbalanced Streams. arXiv:1405.2278
- Dong, M., Liu, M., Jing, C. (2022). One-against-all-based Hellinger Distance Decision Tree. Frontiers of Information Technology & Electronic Engineering
- مقالات مقایسه‌ای در زمینه معیارهای تقسیم برای داده‌های نامتعادل
