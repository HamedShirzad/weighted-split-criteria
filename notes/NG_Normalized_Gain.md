# Normalized Gain (NG)

## اطلاعات کلی
- **منبع اصلی:** A New Criterion in Selection and Discretization of Attributes - Jun et al.
- **سال معرفی:** 1997
- **نویسندگان:** Byung Hwan Jun, Chang Soo Kim, Hong-Yeop Song, Jaihie Kim
- **مجله:** IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 19, No. 12
- **انگیزه:** رفع تعصب معیار Gain نسبت به ویژگی‌های با مقادیر زیاد
- **مزیت کلیدی:** استفاده از پایه لگاریتم n به جای 2 برای تقسیم‌های n-ary

## هدف و کاربرد

معیار Normalized Gain برای رفع محدودیت‌های اساسی معیارهای سنتی Gain و Gain Ratio طراحی شده است:

1. **رفع تعصب نسبت به ویژگی‌های چندمقداره** - معیار Gain معمولاً ویژگی‌هایی با مقادیر زیاد را ترجیح می‌دهد
2. **بهبود عملکرد در تقسیم‌های n-ary** - استفاده از لگاریتم با پایه مناسب
3. **کاهش پیچیدگی درخت** - تولید درخت‌های فشرده‌تر با عمق کمتر
4. **افزایش دقت طبقه‌بندی** - نتایج بهتر در مجموعه داده‌های مختلف

این معیار مخصوصاً برای **ویژگی‌های گسسته با دامنه‌های بزرگ** و **تقسیم‌بندی‌های چندشاخه‌ای** مناسب است.

## فرمول ریاضی

### فرمول اصلی (نسخه مفصل)
$$\text{NG}(A, S, n) = \sum_{j=1}^{k} p_j \log_n p_j - \sum_{i=1}^{n} P_i \sum_{j=1}^{k} p_{ij} \log_n p_{ij}$$

### فرمول ساده‌شده
$$\text{NG}(A, S, n) = \frac{\text{Gain}(A, S)}{\log_2 n}$$

### تعریف متغیرها

| نماد | تعریف |
|------|--------|
| **$\text{NG}$** | امتیاز معیار Normalized Gain |
| **$A$** | ویژگی انتخاب‌شده برای تقسیم |
| **$S$** | مجموعه نمونه‌های گره والد |
| **$n$** | تعداد شاخه‌های تقسیم |
| **$k$** | تعداد کلاس‌های موجود |
| **$p_j$** | احتمال کلاس $j$ در مجموعه $S$ |
| **$P_i$** | نسبت اندازه زیرمجموعه $S_i$ به $S$ |
| **$p_{ij}$** | احتمال کلاس $j$ در زیرمجموعه $S_i$ |

### محاسبات تکمیلی

**نسبت زیرمجموعه‌ها:**
$$P_i = \frac{|S_i|}{|S|}, \quad i = 1, 2, ..., n$$

**احتمالات کلاس در زیرمجموعه‌ها:**
$$p_{ij} = \frac{|S_i \cap c_j|}{|S_i|}, \quad i = 1, 2, ..., n, \quad j = 1, 2, ..., k$$

**معیار Gain استاندارد:**
$$\text{Gain}(A, S) = \text{Entropy}(S) - \sum_{i=1}^{n} P_i \cdot \text{Entropy}(S_i)$$

## مثال محاسبه

فرض کنید یک ویژگی گسسته مجموعه 60 نمونه را به 3 شاخه تقسیم می‌کند:

### داده‌های نمونه:

| شاخه | کلاس A | کلاس B | کلاس C | مجموع |
|------|---------|---------|---------|-------|
| $S_1$ | 15 | 5 | 0 | 20 |
| $S_2$ | 5 | 10 | 5 | 20 |
| $S_3$ | 0 | 5 | 15 | 20 |
| **کل** | 20 | 20 | 20 | 60 |

### گام‌های محاسبه:

**گام ۱:** محاسبه Entropy اولیه
$$\text{Entropy}(S) = -\sum_{j=1}^{3} \frac{20}{60} \log_2 \frac{20}{60} = -3 \times \frac{1}{3} \log_2 \frac{1}{3} = 1.585$$

**گام ۲:** محاسبه Entropy زیرمجموعه‌ها
- $\text{Entropy}(S_1) = -\frac{15}{20} \log_2 \frac{15}{20} - \frac{5}{20} \log_2 \frac{5}{20} = 0.811$
- $\text{Entropy}(S_2) = -3 \times \frac{1}{3} \log_2 \frac{1}{3} = 1.585$
- $\text{Entropy}(S_3) = -\frac{5}{20} \log_2 \frac{5}{20} - \frac{15}{20} \log_2 \frac{15}{20} = 0.811$

**گام ۳:** محاسبه Gain استاندارد
$$\text{Gain}(A, S) = 1.585 - \frac{1}{3}(0.811 + 1.585 + 0.811) = 1.585 - 1.069 = 0.516$$

**گام ۴:** محاسبه Normalized Gain
$$\text{NG}(A, S, 3) = \frac{0.516}{\log_2 3} = \frac{0.516}{1.585} = 0.325$$

## ویژگی‌های فنی

- **پیچیدگی محاسباتی:** $O(n \cdot k)$ جایی که $n$ تعداد شاخه‌ها و $k$ تعداد کلاس‌هاست
- **محدوده مقادیر:** $0 \leq \text{NG} \leq 1$ (در تقسیم‌های باینری)
- **حساسیت به تعداد شاخه‌ها:** کاهش یافته نسبت به Gain معمولی
- **مناسب برای:** تقسیم‌های n-ary و ویژگی‌های چندمقداره

### تفسیر مقادیر:
- **$\text{NG} = 0$**: تقسیم بی‌فایده (هیچ اطلاعاتی اضافه نمی‌شود)
- **$\text{NG}$ بالا**: تقسیم مفید با کاهش قابل توجه عدم قطعیت
- **مقایسه با Gain**: مقادیر کمتری نسبت به Gain اصلی دارد

## مقایسه با سایر معیارها

| جنبه | Normalized Gain | Standard Gain | Gain Ratio | Gini |
|------|-----------------|---------------|------------|------|
| **تعصب به ویژگی‌های چندمقداره** | کم | زیاد | متوسط | متوسط |
| **عملکرد در تقسیم n-ary** | عالی | ضعیف | خوب | متوسط |
| **پایداری عددی** | بالا | متوسط | پایین | بالا |
| **سرعت محاسبه** | متوسط | سریع | کند | سریع |
| **قابلیت تفسیر** | متوسط | بالا | متوسط | بالا |

## مزایا

### 1. رفع تعصب نسبت به ویژگی‌های چندمقداره
- نرمال‌سازی با $\log_2 n$ باعث کاهش مزیت غیرعادلانه ویژگی‌های با مقادیر زیاد می‌شود
- انتخاب بهتر ویژگی‌ها در مراحل اولیه ساخت درخت

### 2. مبنای نظری محکم
- **قضیه ۱:** در شرایط خاص، همیشه بهتر از Gain و Gain Ratio عمل می‌کند
- **قضیه ۲:** زمانی که تقسیم کاملاً خالص باشد، برتری مطلق دارد

### 3. نتایج تجربی مثبت
- آزمایش روی 12 مجموعه داده UCI نشان‌دهنده برتری در اکثر موارد
- کاهش نرخ خطا و پیچیدگی درخت

### 4. سازگاری با الگوریتم‌های موجود
- به آسانی قابل پیاده‌سازی در الگوریتم‌های درخت تصمیم موجود
- نیاز به تغییرات حداقلی در کد

## محدودیت‌ها

### 1. وابستگی به تعداد شاخه‌ها
- عملکرد معیار مستقیماً به تعداد شاخه‌های انتخابی وابسته است
- در تقسیم‌های باینری ممکن است برتری چندانی نداشته باشد

### 2. پیچیدگی محاسباتی اضافی
- نیاز به محاسبه لگاریتم اضافی نسبت به Gain معمولی
- در مجموعه داده‌های بزرگ ممکن است کندتر باشد

### 3. کمبود مطالعات مقایسه‌ای
- نسبت به معیارهای سنتی کمتر مطالعه شده است
- نیاز به ارزیابی بیشتر روی انواع مختلف داده‌ها

### 4. تفسیر پیچیده‌تر
- مفهوم نرمال‌سازی برای کاربران مبتدی دشوار است
- نیاز به آشنایی با مفاهیم نظری اطلاعات

## نکات پیاده‌سازی

### مراقبت‌های ضروری:
- **مدیریت لگاریتم صفر:** اگر $p_{ij} = 0$، آن ترم را نادیده بگیرید
- **تقسیم بر صفر:** اگر $n = 1$، مقدار 0 برگردانید
- **دقت عددی:** از overflow در محاسبه لگاریتم جلوگیری کنید
- **انتخاب پایه لگاریتم:** مطمئن شوید از پایه مناسب استفاده می‌کنید

### بهینه‌سازی:
- کش کردن محاسبات Entropy برای استفاده مجدد
- استفاده از log-sum-exp trick برای پایداری عددی
- محاسبه موثر آنتروپی با استفاده از NumPy

### الگوی پیاده‌سازی:
محاسبه Gain اول، سپس نرمال‌سازی

gain_value = calculate_information_gain(y_parent, y_children)
normalized_gain = gain_value / np.log2(n_branches)

## کد شبه


def normalized_gain(y_left, y_right, n_branches=2):
"""
محاسبه امتیاز معیار Normalized Gain

Parameters:
y_left: آرایه برچسب‌های کلاس برای شاخه چپ
y_right: آرایه برچسب‌های کلاس برای شاخه راست
n_branches: تعداد شاخه‌های تقسیم (پیش‌فرض: 2)

Returns:
NG: امتیاز Normalized Gain
"""

# ترکیب داده‌های والد
y_parent = concatenate([y_left, y_right])
n_total = len(y_parent)

# بررسی شرایط خاص
if n_total == 0 or n_branches <= 1:
    return 0.0

# محاسبه آنتروپی والد
parent_entropy = calculate_entropy(y_parent)

# محاسبه آنتروپی زیرمجموعه‌ها
n_left = len(y_left)
n_right = len(y_right)

weighted_entropy = 0.0
if n_left > 0:
    weighted_entropy += (n_left / n_total) * calculate_entropy(y_left)
if n_right > 0:
    weighted_entropy += (n_right / n_total) * calculate_entropy(y_right)

# محاسبه Information Gain استاندارد
information_gain = parent_entropy - weighted_entropy

# نرمال‌سازی با log_2(n)
normalized_gain_value = information_gain / log2(n_branches)

return normalized_gain_value

def calculate_entropy(y):
"""محاسبه آنتروپی یک مجموعه داده"""
if len(y) == 0:
return 0.0

# شمارش فراوانی کلاس‌ها
class_counts = {}
for label in y:
    class_counts[label] = class_counts.get(label, 0) + 1

# محاسبه آنتروپی
entropy = 0.0
n_samples = len(y)

for count in class_counts.values():
    if count > 0:
        probability = count / n_samples
        entropy -= probability * log2(probability)

return entropy


```

## منابع
- Jun, B.H., Kim, C.S., Song, H.Y., Kim, J. (1997). A New Criterion in Selection and Discretization of Attributes for the Generation of Decision Trees. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(12), 1371-1375
- Quinlan, J.R. (1986). Induction of Decision Trees. Machine Learning, 1(1), 81-106
- Quinlan, J.R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers
- مطالعات مقایسه‌ای معیارهای تقسیم در یادگیری ماشین
