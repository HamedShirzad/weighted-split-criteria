# Gain Ratio (Quinlan Gain, Information Gain Ratio)

## اطلاعات کلی
- **منبع اصلی:** Quinlan, J.R. (1993), C4.5: Programs for Machine Learning
- **سال معرفی:** 1993  
- **نویسنده:** J. Ross Quinlan  
- **الگوریتم مرجع:** C4.5  
- **انگیزه:** رفع تعصب معیار Information Gain به ویژگی‌های با تعداد مقدار زیاد  
- **مزیت کلیدی:** تقسیم بهینه‌تر، نرمال‌سازی تاثیر پراکندگی تقسیم

## هدف و کاربرد
Gain Ratio به عنوان نسخه بهبود یافته Information Gain برای انتخاب ویژگی و تقسیم گره‌های درخت تصمیم کاربرد دارد تا:
- ویژگی‌های با تعداد مقدار زیاد (high-cardinality) را بیش‌ از حد ترجیح ندهد.
- انتخاب ویژگی را پایدار، معنادار و واقعی‌تر کند.

## فرمول ریاضی

### ۱) **Information Gain (IG):**
\[
IG(A) = Entropy(S) - \sum_{i=1}^v \frac{|S_i|}{|S|} Entropy(S_i)
\]

### ۲) **Intrinsic Value (IV) یا Split Info:**
\[
IV(A) = -\sum_{i=1}^v \frac{|S_i|}{|S|} \log_2 \left(\frac{|S_i|}{|S|}\right)
\]

### ۳) **Gain Ratio (GR):**
\[
GR(A) = \frac{IG(A)}{IV(A)}
\]
- در صورت $IV(A)=0$، تقسیم نادیده گرفته می‌شود.

## الگوریتم
1. برای هر ویژگی $A$ در گره:
   - IG(A) و IV(A) و GR(A) را محاسبه کن.
   - ویژگی ای که بالاترین GR و IG مثبت دارد را انتخاب کن.
2. تقسیم و ادامه رشد درخت تا معیار stopping فعال شود.

## مثال عددی
فرض بر جمع $|S|=14$،
فرض زیرگروه‌ها $|S_1|=5, |S_2|=5, |S_3|=4$، Entropy(S)=0.94  
IG(A)=0.246  
IV(A)=$-(5/14 \log_2(5/14)+...)=1.577$  
GR(A)=0.246/1.577=0.156

## ویژگی‌های فنی
- مناسب in multiway splits و ویژگی‌های با مقدار زیاد  
- درخت‌های متوازن‌تر نسبت به IG  
- پیچیدگی محاسبه کمی بیشتر از IG (به ازای هر split باید IV هم محاسبه شود)

## مقایسه با سایر معیارها

| جنبه               | Information Gain | Gain Ratio        | Gini       |
|--------------------|-----------------|-------------------|------------|
| **تعصب high-card** | زیاد            | رفع شده           | خیلی کم    |
| **محاسبه**         | سریع            | متوسط             | خیلی سریع  |
| **کاربرد**         | ID3             | C4.5              | CART       |

## مزایا
- حذف تعصب ویژگی‌های چندمقداره
- تقسیمات متوازن‌تر و واقعی‌تر
- استاندارد مرجع در درخت‌های C4.5

## محدودیت‌ها
- اگر IV بیش از حد کوچک شود تقسیم نادیده گرفته می‌شود (ممکن است برخی ویژگی‌های واقعا عالی انتخاب نشوند)
- برای داده‌های بسیار نامتوازن در بعضی splitها عدد GR می‌تواند بی‌معنا شود (نیاز به threshold برای IV)

## نکات پیاده‌سازی
- همیشه IG را اول محاسبه کن، اگر IG=۰ شد، تقسیم بی‌اعتبار است
- اگر IV=۰ شد آن تقسیم را اصلا لحاظ نکن
- برای تصادفی نبودن انتخاب، توصیه می‌شود همیشه تقسیم با IG>۰ و بیشترین GR را انتخاب کنی

## کد شبه
def gain_ratio(y_parent, y_branches):
# محاسبه Entropy والد
# محاسبه میانگین Entropy branches با وزن مناسب
# IG = ...
# IV = ...
# if IV==0: return 0
# return IG/IV

```
## منابع
- Quinlan, J.R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann
- Han, J., & Kamber, M. (2006). Data Mining: Concepts and Techniques. Morgan Kaufmann
- سایر مراجع استاندارد یادگیری ماشین
