# Marshall Criterion (MARSH)

## اطلاعات کلی
- **منبع اصلی:** Partitioning Methods for Classification and Decision Making in Medicine
- **سال معرفی:** 1986
- **نویسنده:** R.J. Marshall (Leeds University, UK)
- **مجله:** Statistics in Medicine, Vol. 5, 517-526
- **انگیزه:** ایجاد تقسیم‌بندی منظم با در نظر گیری دانش پیشین در تصمیم‌گیری پزشکی
- **مزیت کلیدی:** کنترل overfitting از طریق محدود کردن فضای جستجو به Regular Partitions

## هدف و کاربرد

معیار Marshall به عنوان **جایگزین بهبود یافته Recursive Partitioning** برای تصمیم‌گیری در پزشکی طراحی شده است:

1. **تقسیم‌بندی منظم (Regular Partitioning)** - محدود کردن تقسیم‌ها به ساختارهای منطقی و تفسیرپذیر
2. **در نظر گیری دانش پیشین** - انعکاس باورهای تخصصی و جهت مورد انتظار روابط
3. **جلوگیری از overfitting** - استفاده همزمان از تمام داده‌ها بدون تقسیم به زیرگروه‌های کوچک
4. **تفسیرپذیری بالا** - تولید قوانین Boolean ساده و قابل درک برای متخصصان

این معیار مخصوصاً برای **حوزه‌های تخصصی** مانند پزشکی، مهندسی، و مالی مناسب است که دانش پیشین قوی وجود دارد.

## فرمول ریاضی

### فرمول اصلی کاهش تنوع Gini
$$G = \frac{(C \times D)}{N} - \frac{(a \times b)}{A} - \frac{(c \times d)}{B}$$

### فرمول نرمال‌سازی شده
$$G^* = G \times A \times B$$

### تعریف متغیرها بر اساس جدول Contingency

برای یک تقسیم باینری، جدول 2×2 زیر تشکیل می‌شود:

| تقسیم | y=0 (کلاس منفی) | y=1 (کلاس مثبت) | مجموع ردیف |
|-------|------------------|------------------|-------------|
| **A₀** | a | b | A = a + b |
| **A₁** | c | d | B = c + d |
| **مجموع ستون** | C = a + c | D = b + d | N = a + b + c + d |

| نماد | تعریف |
|------|--------|
| **G** | کاهش تنوع Gini |
| **G*** | کاهش تنوع نرمال‌سازی شده |
| **A₀** | ناحیه تقسیم اول (مثبت) |
| **A₁** | ناحیه تقسیم دوم (منفی) |
| **a, b, c, d** | فراوانی‌های جدول contingency |
| **A, B** | مجموع نمونه‌ها در هر ناحیه |
| **C, D** | مجموع نمونه‌ها از هر کلاس |
| **N** | تعداد کل نمونه‌ها |

### تعریف Regular Partition
تقسیم منظم به صورت زیر تعریف می‌شود:

$$A_0 = I_1 \cup I_2 \cup \ldots \cup I_\phi$$

جایی که هر $I_s$ تقاطع مجموعه‌ای از ویژگی‌های **غیرمکمل** است:
$$I_s = X_{j_1} \cap X_{j_2} \cap \ldots \cap X_{j_k}$$

## مثال محاسبه

فرض کنید در تشخیص بیماری سرطان دستگاه گوارش، 200 بیمار را بررسی می‌کنیم:

### داده‌های نمونه:

**تقسیم منظم تعریف شده:**
A₀ = (بدون کاهش وزن ∩ بدون تغییر عادت دفع) ∪ (بدون کاهش اشتها ∩ دفع طبیعی)
A₁ = مکمل A₀

**جدول contingency نتیجه:**

| تقسیم | خوش‌خیم (y=0) | سرطان (y=1) | مجموع |
|-------|----------------|--------------|--------|
| **A₀** | 96 | 24 | 120 |
| **A₁** | 14 | 66 | 80 |
| **مجموع** | 110 | 90 | 200 |

### گام‌های محاسبه:

**گام ۱:** شناسایی متغیرها
- a = 96, b = 24, c = 14, d = 66
- A = 120, B = 80, C = 110, D = 90, N = 200

**گام ۲:** محاسبه کاهش تنوع Gini
$$G = \frac{110 \times 90}{200} - \frac{96 \times 24}{120} - \frac{14 \times 66}{80}$$

$$G = \frac{9900}{200} - \frac{2304}{120} - \frac{924}{80}$$

$$G = 49.5 - 19.2 - 11.55 = 18.75$$

**گام ۳:** محاسبه G نرمال‌سازی شده
$$G^* = 18.75 \times 120 \times 80 = 180,000$$

**گام ۴:** تفسیر نتایج
- **دقت پیش‌بینی برای خوش‌خیم:** 96/110 = 87.3%
- **دقت پیش‌بینی برای سرطان:** 66/90 = 73.3%
- **دقت کلی:** (96+66)/200 = 81%

## ویژگی‌های فنی

- **پیچیدگی محاسباتی:** $O(2^k)$ جایی که k تعداد ویژگی‌های باینری است
- **نوع داده:** مناسب برای ویژگی‌های گسسته و باینری
- **محدوده مقادیر:** G می‌تواند مثبت، منفی، یا صفر باشد
- **کنترل overfitting:** از طریق Regular Partitions
- **تفسیرپذیری:** بالا - خروجی به شکل قوانین Boolean

### خصوصیات ریاضی:
- **مقایسه با تصادفی:** G > 0 نشان‌دهنده بهتر بودن از تقسیم تصادفی
- **حداکثر کاهش تنوع:** زمانی که تقسیم کاملاً جداکننده باشد
- **تعادل کلاس‌ها:** در نظر گیری نابرابری توزیع کلاس‌ها

### تفسیر مقادیر:
- **G = 0**: تقسیم معادل با تصادفی (بی‌فایده)
- **G > 0**: تقسیم بهتر از تصادفی (مفید)
- **G < 0**: تقسیم بدتر از تصادفی (مضر)

## مقایسه با سایر معیارها

| جنبه | Marshall | Recursive Partitioning | Information Gain | Gini Impurity |
|------|----------|------------------------|------------------|---------------|
| **استفاده از داده** | همزمان تمام داده‌ها | تقسیم زیرگروه‌ها | تقسیم زیرگروه‌ها | تقسیم زیرگروه‌ها |
| **دانش پیشین** | اجباری | اختیاری | ندارد | ندارد |
| **فضای جستجو** | محدود (Regular) | نامحدود | نامحدود | نامحدود |
| **تفسیرپذیری** | بسیار بالا | متوسط | متوسط | پایین |
| **کنترل overfitting** | بالا | پایین | پایین | پایین |
| **کاربرد تخصصی** | عالی | متوسط | محدود | محدود |

## مزایا

### 1. کنترل قدرتمند overfitting
- **محدودسازی فضای جستجو:** تنها تقسیم‌های منطقی و منظم در نظر گرفته می‌شوند
- **استفاده از تمام داده‌ها:** بدون تقسیم به زیرگروه‌های کوچک
- **جلوگیری از تقسیم‌های تصادفی:** کنترل از طریق دانش پیشین

### 2. در نظر گیری دانش تخصصی
- **انعکاس باورهای پیشین:** ساختار تقسیم منطبق با دانش حوزه
- **جهت‌دار بودن:** در نظر گیری جهت مورد انتظار روابط
- **سازگاری با تجربه:** عدم تولید روابط متناقض با دانش موجود

### 3. تفسیرپذیری عالی
- **قوانین Boolean واضح:** خروجی به شکل AND/OR واضح
- **کاهش پیچیدگی:** امکان ساده‌سازی عبارات Boolean
- **ارتباط با منطق تخصصی:** سازگاری با فرآیند تصمیم‌گیری طبیعی

### 4. عملکرد قابل رقابت
- **نتایج مشابه:** دقت مشابه با Logistic Regression
- **پایداری بالا:** کمتر متأثر از نویز داده‌ها
- **قابلیت تعمیم:** عملکرد خوب روی داده‌های جدید

## محدودیت‌ها

### 1. وابستگی شدید به دانش پیشین
- **نیاز به تخصص حوزه:** بدون دانش تخصصی قابل استفاده نیست
- **تعصب احتمالی:** ممکن است باورهای نادرست را تقویت کند
- **محدودیت در حوزه‌های جدید:** کاربرد محدود در مسائل بدون دانش پیشین

### 2. پیچیدگی تعریف Regular Partitions
- **چالش مدل‌سازی:** تبدیل دانش تخصصی به ساختار ریاضی دشوار است
- **انفجار ترکیبی:** تعداد تقسیم‌های ممکن همچنان زیاد باشد
- **نیاز به بررسی دستی:** انتخاب نهایی تقسیم نیاز به تصمیم تخصصی دارد

### 3. محدودیت در نوع داده‌ها
- **ویژگی‌های پیوسته:** نیاز به discretization قبل از استفاده
- **داده‌های پیچیده:** مناسب برای ساختارهای ساده
- **مقیاس‌بندی:** مشکل در کار با ویژگی‌های چندمقیاسه

### 4. عدم انعطاف‌پذیری
- **ساختار صلب:** بعد از تعریف Regular Partitions قابل تغییر نیست
- **عدم یادگیری:** نمی‌تواند ساختار جدیدی از داده‌ها کشف کند
- **وابستگی به انسان:** نیاز مستمر به مداخله تخصصی

## نکات پیاده‌سازی

### مراقبت‌های ضروری:
- **تعریف دقیق Regular Partitions:** ساختار Boolean باید منطقی و کامل باشد
- **مدیریت ترکیبات:** کنترل انفجار ترکیبی در تولید partitions
- **اعتبارسنجی دانش پیشین:** بررسی سازگاری باورهای تخصصی با داده‌ها
- **محاسبه دقیق G:** جلوگیری از خطاهای عددی در محاسبات

### بهینه‌سازی:
- کش کردن محاسبات contingency table برای partitions مختلف
- استفاده از bit manipulation برای عملیات Boolean سریع
- محاسبه موازی برای ارزیابی multiple partitions
- استفاده از pruning برای کاهش فضای جستجو

### الگوی پیاده‌سازی:
تعریف Regular Partitions

regular_partitions = define_partitions_from_domain_knowledge()
ارزیابی هر partition

best_g = -infinity
best_partition = None

for partition in regular_partitions:
contingency_table = create_table(data, partition)
g_value = calculate_gini_reduction(contingency_table)
if g_value > best_g:
    best_g = g_value
    best_partition = partition

return best_partition, best_g

## کد شبه

def marshall_criterion(X, y, regular_partitions):
"""
محاسبه معیار Marshall برای مجموعه Regular Partitions
Parameters:
X: ماتریس ویژگی‌ها
y: برچسب‌های کلاس (0 یا 1)
regular_partitions: لیست تقسیم‌های منظم تعریف شده

Returns:
best_partition: بهترین تقسیم یافت شده
best_g: بالاترین مقدار G
"""

best_g = -float('inf')
best_partition = None
n_total = len(y)

# محاسبه مقادیر پایه
C = sum(y == 0)  # تعداد کل کلاس منفی
D = sum(y == 1)  # تعداد کل کلاس مثبت

for partition in regular_partitions:
    # ایجاد ماسک برای تقسیم
    mask_A0 = evaluate_partition(X, partition)
    mask_A1 = ~mask_A0
    
    # محاسبه جدول contingency
    a = sum((y == 0) & mask_A0)  # منفی در A0
    b = sum((y == 1) & mask_A0)  # مثبت در A0
    c = sum((y == 0) & mask_A1)  # منفی در A1
    d = sum((y == 1) & mask_A1)  # مثبت در A1
    
    A = a + b  # مجموع A0
    B = c + d  # مجموع A1
    
    # بررسی معتبر بودن تقسیم
    if A == 0 or B == 0:
        continue
    
    # محاسبه G
    g_value = calculate_g(a, b, c, d, A, B, C, D, n_total)
    
    if g_value > best_g:
        best_g = g_value
        best_partition = partition

return best_partition, best_g
def calculate_g(a, b, c, d, A, B, C, D, N):
"""محاسبه کاهش تنوع Gini"""
def calculate_g(a, b, c, d, A, B, C, D, N):
"""محاسبه کاهش تنوع Gini"""
# فرمول اصلی Marshall
term1 = (C * D) / N
term2 = (a * b) / A if A > 0 else 0
term3 = (c * d) / B if B > 0 else 0

g_value = term1 - term2 - term3

return g_value
def evaluate_partition(X, partition):
"""ارزیابی تقسیم منظم روی داده‌ها"""
mask = np.zeros(len(X), dtype=bool)

# partition شامل مجموعه‌ای از intersections است
for intersection in partition:
    temp_mask = np.ones(len(X), dtype=bool)
    
    # هر intersection شامل مجموعه‌ای از شرایط است
    for condition in intersection:
        feature, value = condition
        temp_mask &= (X[:, feature] == value)
    
    mask |= temp_mask

return mask
def define_medical_partitions():
"""مثال تعریف Regular Partitions برای تشخیص پزشکی"""
# تعریف ویژگی‌ها
# 0: کاهش وزن، 1: تغییر اشتها، 2: تغییر دفع، 3: درد شکم

partitions = []

# Partition 1: بدون علائم تهاجمی
partition1 = [
    [(0, 0), (2, 0)],  # بدون کاهش وزن و بدون تغییر دفع
    [(1, 0), (3, 0)]   # بدون کاهش اشتها و بدون درد شکم
]
partitions.append(partition1)

# Partition 2: علائم خفیف
partition2 = [
    [(0, 0), (1, 0)],  # بدون کاهش وزن و اشتها
    [(2, 0), (3, 0)]   # بدون تغییر دفع و درد
]
partitions.append(partition2)

return partitions

```

## منابع
- Marshall, R.J. (1986). Partitioning Methods for Classification and Decision Making in Medicine. Statistics in Medicine, 5, 517-526
- Breiman, L., Friedman, J., Stone, C.J., Olshen, R.A. (1984). Classification and Regression Trees. CRC Press
- Quinlan, J.R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers
- مقالات کاربردی در تصمیم‌گیری پزشکی و سیستم‌های تشخیص
