# Chi-Squared Criterion (Χ²)

## اطلاعات کلی
- **منبع اصلی:** Expert Systems Ruleuction with Statistical Data – John Mingers (1987)  
- **سال معرفی:** 1987  
- **نویسنده:** John Mingers  
- **مجله:** Journal of the Operational Research Society  
- **انگیزه:** استفاده از آزمون آماری χ² برای کنترل overfitting در تقسیم‌ درخت تصمیم  
- **مزیت کلیدی:** توقف خودکار تقسیم‌ها بر اساس معنی‌داری آماری (p-value)

## هدف و کاربرد
Chi-Squared Criterion برای یافتن تقسیم‌هایی طراحی شده که از نظر آماری معنادار باشند. این روش:
1. برای هر ویژگی کاندید، جدول contingency بین شاخه‌ها و برچسب‌ها را می‌سازد.  
2. با محاسبه χ² و p-value، تقسیم‌هایی که p < α دارند را معتبر می‌داند.  
3. از تقسیم‌های تصادفی یا بی‌اهمیت جلوگیری می‌کند و overfitting کاهش می‌یابد.

## فرمول ریاضی
برای یک تقسیم با \(r\) شاخه و \(c\) کلاس:
\[
\chi^2 \;=\;\sum_{i=1}^{r}\sum_{j=1}^{c}\frac{(O_{ij}-E_{ij})^2}{E_{ij}}
\]
که در آن:
- \(O_{ij}\) فراوانی مشاهده‌شده در شاخه \(i\) و کلاس \(j\).  
- \(E_{ij} = \dfrac{x_{i\cdot}\,x_{\cdot j}}{N}\) فراوانی مورد انتظار، با  
  \(x_{i\cdot}\) مجموع ردیف \(i\)،  
  \(x_{\cdot j}\) مجموع ستون \(j\)،  
  \(N\) تعداد کل نمونه‌ها.  
- درجه آزادی:  \(\;df = (r-1)\times(c-1)\).

## الگوریتم گام‌به‌گام
1. برای هر ویژگی \(A\):  
   a. ساخت جدول contingency \((r\times c)\).  
   b. محاسبه \(E_{ij}\).  
   c. محاسبه \(\chi^2\).  
   d. محاسبه \(p\)-value از توزیع \(\chi^2_{df}\).  
2. از میان ویژگی‌ها، آن را انتخاب کن که بزرگ‌ترین \(\chi^2\) و کوچک‌ترین \(p\) (و \(p<\alpha\)) دارد.  
3. اگر هیچ ویژگی معنادار نبود (\(p\ge\alpha\)) در آن گره تقسیم متوقف می‌شود.

## مثال عددی
فرض تقسیم باینری روی دو کلاس A و B:
| شاخه | A (O) | B (O) | جمع |
|------|-------|-------|-----|
| چپ  | 30    | 10    | 40  |
| راست| 20    | 40    | 60  |
| کل  | 50    | 50    |100  |

- \(E_{11} = 40\times50/100=20\)، \(E_{12}=40\times50/100=20\)  
- \(E_{21}=60\times50/100=30\)، \(E_{22}=60\times50/100=30\)  
- \(\chi^2 = (30-20)^2/20 + (10-20)^2/20 + (20-30)^2/30 + (40-30)^2/30 = 10/20 + 100/20 +100/30 +100/30 =0.5+5+3.33+3.33=12.16\)  
- \(df=(2-1)(2-1)=1\)، آستانه \(\chi^2_{0.05,1}=3.84\)، چون \(12.16>3.84\) تقسیم معنادار است.

## ویژگی‌های فنی
- پیچیدگی محاسباتی: \(O(r\times c)\)  
- نیاز به \(E_{ij}\ge5\) برای اعتبار آزمون  
- سطح معنی‌داری \(\alpha\) قابل تنظیم (معمولاً 0.05)

## مقایسه با سایر معیارها

| جنبه                          | Chi-Squared         | G-Statistic     | Information Gain | Gini Impurity |
|-------------------------------|---------------------|-----------------|------------------|---------------|
| آزمون معناداری               | دارد               | دارد            | ندارد           | ندارد        |
| حساسیت به فراوانی کم         | بالا                | کمتر           | متوسط           | پایین        |
| تعصب به ویژگی‌های چندشاخه   | بالا بدون نرمال‌سازی| متوسط          | بالا            | کم           |
| پیچیدگی محاسبه               | \(O(r\times c)\)    | \(O(r\times c)\)| \(O(n\log n)\)   | \(O(n)\)     |

## مزایا
- استفاده از p-value برای کنترل overfitting  
- مبنای آماری قوی و شناخته‌شده  
- امکان نرمال‌سازی (\(\chi^2/df\)) برای کاهش تعصب

## محدودیت‌ها
- حساسیت به فراوانی‌های کم (\(E_{ij}<5\))  
- تمایل به درخت‌های عمیق‌تر  
- نیاز به تنظیم دقیق \(\alpha\)

## نکات پیاده‌سازی
- بررسی \(E_{ij}\ge5\) قبل از آزمون  
- محاسبه سریع با NumPy برای جداول بزرگ  
- تنظیم \(\alpha\) برای کنترل عمق درخت

## کد شبه
def chi_squared_criterion(y_left, y_right, alpha=0.05):
# ساخت contingency table
# محاسبه E_ij
# محاسبه chi2
# محاسبه p-value
# بازگشت (chi2, p_value, is_significant)

```

## منابع
- Mingers, J. (1987). Expert Systems Rule Induction with Statistical Data. Journal of the Operational Research Society.  
- Agresti, A. (2002). Categorical Data Analysis. John Wiley & Sons.  
- Breiman, L. et al. (1984). Classification and Regression Trees. CRC Press.  
